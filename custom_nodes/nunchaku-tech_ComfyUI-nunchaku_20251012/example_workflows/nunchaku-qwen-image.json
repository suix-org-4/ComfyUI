{
  "id": "b189367d-aede-4a4c-b277-7fa3bc10f655",
  "revision": 0,
  "last_node_id": 14,
  "last_link_id": 11,
  "nodes": [
    {
      "id": 4,
      "type": "EmptySD3LatentImage",
      "pos": [
        1011.4683227539062,
        875.4708251953125
      ],
      "size": [
        270,
        106
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "links": [
            10
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "EmptySD3LatentImage"
      },
      "widgets_values": [
        1328,
        1328,
        1
      ]
    },
    {
      "id": 5,
      "type": "CLIPTextEncode",
      "pos": [
        1013.8109130859375,
        643.9240112304688
      ],
      "size": [
        425.27801513671875,
        180.6060791015625
      ],
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 4
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            9
          ]
        }
      ],
      "title": "CLIP Text Encode (Negative Prompt)",
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        " "
      ],
      "color": "#322",
      "bgcolor": "#533"
    },
    {
      "id": 8,
      "type": "ModelSamplingAuraFlow",
      "pos": [
        1019.0407104492188,
        323.7497863769531
      ],
      "size": [
        270,
        58
      ],
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 11
        }
      ],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            7
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "ModelSamplingAuraFlow"
      },
      "widgets_values": [
        3.1000000000000005
      ]
    },
    {
      "id": 10,
      "type": "Note",
      "pos": [
        1020.6795043945312,
        179.56192016601562
      ],
      "size": [
        261.87744140625,
        102.99000549316406
      ],
      "flags": {},
      "order": 1,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "Increase the shift if you get too many blury/dark/bad images. Decrease if you want to try increasing detail."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 11,
      "type": "Note",
      "pos": [
        1470.9071044921875,
        750.8844604492188
      ],
      "size": [
        307.4002380371094,
        127.38092803955078
      ],
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "Set cfg to 1.0 for a speed boost at the cost of consistency. Samplers like res_multistep work pretty well at cfg 1.0\n\nThe official number of steps is 50 but I think that's too much. Even just 10 steps seems to work."
      ],
      "color": "#432",
      "bgcolor": "#653"
    },
    {
      "id": 12,
      "type": "KSampler",
      "pos": [
        1463.8109130859375,
        441.92401123046875
      ],
      "size": [
        315,
        262
      ],
      "flags": {},
      "order": 10,
      "mode": 0,
      "inputs": [
        {
          "name": "model",
          "type": "MODEL",
          "link": 7
        },
        {
          "name": "positive",
          "type": "CONDITIONING",
          "link": 8
        },
        {
          "name": "negative",
          "type": "CONDITIONING",
          "link": 9
        },
        {
          "name": "latent_image",
          "type": "LATENT",
          "link": 10
        }
      ],
      "outputs": [
        {
          "name": "LATENT",
          "type": "LATENT",
          "slot_index": 0,
          "links": [
            1
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "KSampler"
      },
      "widgets_values": [
        279044145809423,
        "randomize",
        20,
        2.5,
        "euler",
        "simple",
        1
      ]
    },
    {
      "id": 2,
      "type": "VAEDecode",
      "pos": [
        1469.6492919921875,
        331.552978515625
      ],
      "size": [
        210,
        46
      ],
      "flags": {},
      "order": 11,
      "mode": 0,
      "inputs": [
        {
          "name": "samples",
          "type": "LATENT",
          "link": 1
        },
        {
          "name": "vae",
          "type": "VAE",
          "link": 2
        }
      ],
      "outputs": [
        {
          "name": "IMAGE",
          "type": "IMAGE",
          "slot_index": 0,
          "links": [
            3
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "VAEDecode"
      },
      "widgets_values": []
    },
    {
      "id": 9,
      "type": "CLIPTextEncode",
      "pos": [
        1015.8109130859375,
        440.92401123046875
      ],
      "size": [
        422.84503173828125,
        164.31304931640625
      ],
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 6
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "slot_index": 0,
          "links": [
            8
          ]
        }
      ],
      "title": "CLIP Text Encode (Positive Prompt)",
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": [
        "cute anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair between eyes and large blue eyes blonde colored eyelashes chubby wearing oversized clothes summer uniform long blue maxi skirt muddy clothes happy sitting on the side of the road in a run down dark gritty cyberpunk city with neon and a crumbling skyscraper in the rain at night while dipping her feet in a river of water she is holding a sign that says \"Nunchaku is the fastest\" written in cursive"
      ],
      "color": "#232",
      "bgcolor": "#353"
    },
    {
      "id": 1,
      "type": "VAELoader",
      "pos": [
        620.7332153320312,
        720.8971557617188
      ],
      "size": [
        330,
        60
      ],
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "VAE",
          "type": "VAE",
          "slot_index": 0,
          "links": [
            2
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "VAELoader"
      },
      "widgets_values": [
        "qwen_image_vae.safetensors"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 7,
      "type": "CLIPLoader",
      "pos": [
        592.35400390625,
        572.7418823242188
      ],
      "size": [
        380,
        106
      ],
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "slot_index": 0,
          "links": [
            4,
            6
          ]
        }
      ],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51",
        "Node name for S&R": "CLIPLoader"
      },
      "widgets_values": [
        "qwen_2.5_vl_7b_fp8_scaled.safetensors",
        "qwen_image",
        "default"
      ],
      "color": "#223",
      "bgcolor": "#335"
    },
    {
      "id": 13,
      "type": "NunchakuQwenImageDiTLoader",
      "pos": [
        604.699951171875,
        448.6050720214844
      ],
      "size": [
        320.1197204589844,
        58
      ],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "MODEL",
          "type": "MODEL",
          "links": [
            11
          ]
        }
      ],
      "properties": {
        "aux_id": "nunchaku-tech/ComfyUI-nunchaku",
        "ver": "17c9fb8260879267f705d4b7b25cc700d794a826",
        "Node name for S&R": "NunchakuQwenImageDiTLoader"
      },
      "widgets_values": [
        "svdq-int4_r32-qwen-image.safetensors"
      ]
    },
    {
      "id": 3,
      "type": "SaveImage",
      "pos": [
        1805.813720703125,
        196.22177124023438
      ],
      "size": [
        821.0496215820312,
        871.7067260742188
      ],
      "flags": {},
      "order": 12,
      "mode": 0,
      "inputs": [
        {
          "name": "images",
          "type": "IMAGE",
          "link": 3
        }
      ],
      "outputs": [],
      "properties": {
        "cnr_id": "comfy-core",
        "ver": "0.3.51"
      },
      "widgets_values": [
        "ComfyUI"
      ]
    },
    {
      "id": 14,
      "type": "MarkdownNote",
      "pos": [
        573.8006591796875,
        235.31703186035156
      ],
      "size": [
        387.47528076171875,
        145.53501892089844
      ],
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [],
      "outputs": [],
      "properties": {},
      "widgets_values": [
        "Download the models from [Hugging Face](https://huggingface.co/nunchaku-tech/nunchaku-qwen-image) or [ModelScope](https://modelscope.cn/models/nunchaku-tech/nunchaku-qwen-image), and place them in the `models/diffusion_models` directory.\n\n- **50-series GPUs:** use the **FP4** models  \n- **Other GPUs:** use the **INT4** models  \n\nNote: \n- `r128` models provide higher quality than `r32`, but run slightly slower. \n- LoRA support is not available now but will come soon.\n"
      ],
      "color": "#432",
      "bgcolor": "#653"
    }
  ],
  "links": [
    [
      1,
      12,
      0,
      2,
      0,
      "LATENT"
    ],
    [
      2,
      1,
      0,
      2,
      1,
      "VAE"
    ],
    [
      3,
      2,
      0,
      3,
      0,
      "IMAGE"
    ],
    [
      4,
      7,
      0,
      5,
      0,
      "CLIP"
    ],
    [
      6,
      7,
      0,
      9,
      0,
      "CLIP"
    ],
    [
      7,
      8,
      0,
      12,
      0,
      "MODEL"
    ],
    [
      8,
      9,
      0,
      12,
      1,
      "CONDITIONING"
    ],
    [
      9,
      5,
      0,
      12,
      2,
      "CONDITIONING"
    ],
    [
      10,
      4,
      0,
      12,
      3,
      "LATENT"
    ],
    [
      11,
      13,
      0,
      8,
      0,
      "MODEL"
    ]
  ],
  "groups": [],
  "config": {},
  "extra": {
    "ds": {
      "scale": 1.0078390287890118,
      "offset": [
        -216.72669075214156,
        -78.69797051371079
      ]
    },
    "frontendVersion": "1.25.9"
  },
  "version": 0.4
}
