"""
ðŸ“ Phoneme Text Normalizer Node

A universal text preprocessing node for multilingual TTS that handles special characters,
phonemization, and text normalization for languages like Polish, German, French, Spanish, etc.

This node provides various text transformation methods to make multilingual text compatible
with TTS models that may have been trained on different character sets or phoneme representations.
"""

import re
import unicodedata
from typing import Dict, List, Tuple, Optional

class PhonemeTextNormalizer:
    """
    Universal text normalizer for multilingual TTS preprocessing.
    Handles special characters, phonemization, and text transformations.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {
                    "multiline": True,
                    "default": "Enter your text here. Examples:\nPolish: CzeÅ›Ä‡, jak siÄ™ masz?\nGerman: SchÃ¶ne GrÃ¼ÃŸe aus MÃ¼nchen!\nFrench: Bonjour, comment allez-vous?",
                    "tooltip": "Input text to normalize for TTS processing.\n\nSupports multilingual text with special characters like:\nâ€¢ Polish: Ä…, Ä‡, Ä™, Å‚, Å„, Ã³, Å›, Åº, Å¼\nâ€¢ German: Ã¤, Ã¶, Ã¼, ÃŸ\nâ€¢ French: Ã , Ã©, Ãª, Ã§, etc.\nâ€¢ And many other languages\n\nConnect this to any TTS engine for improved pronunciation."
                }),
                "method": (["Pass-through", "Unicode Decomposition", "IPA Phonemization", "Character Mapping"], {
                    "default": "Unicode Decomposition",
                    "tooltip": "Text processing method to apply:\n\nâ€¢ Pass-through: No processing (original text)\nâ€¢ Unicode Decomposition: Ä…â†’aÌ§, Ä‡â†’Ä‡ (recommended for most cases)\nâ€¢ IPA Phonemization: Full phonetic conversion (Ä…â†’É”Ìƒ, requires espeak)\nâ€¢ Character Mapping: ASCII fallback (Ä…â†’a, Ä‡â†’c)\n\nStart with Unicode Decomposition - it fixes most pronunciation issues."
                }),
                "language": (["Auto-detect", "Polish", "German", "French", "Spanish", "Portuguese", "Italian", "Czech", "Slovak", "Hungarian", "Norwegian", "Swedish", "Danish", "Finnish", "Dutch", "English"], {
                    "default": "Auto-detect",
                    "tooltip": "Language for processing (affects IPA Phonemization):\n\nâ€¢ Auto-detect: Automatically detects language from text\nâ€¢ Manual selection: Choose specific language for better accuracy\n\nLanguage detection looks for special characters:\nâ€¢ Polish: Ä…, Ä™, Ä‡, Å‚, etc.\nâ€¢ German: Ã¤, Ã¶, Ã¼, ÃŸ\nâ€¢ French: Ã , Ã©, Ãª, Ã§, etc.\n\nOnly affects IPA Phonemization method."
                }),
                "show_debug": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Show Before/After",
                    "label_off": "Hide Debug Info",
                    "tooltip": "Show debug information in console:\n\nâ€¢ Original vs normalized text comparison\nâ€¢ Character-by-character changes\nâ€¢ Detected language (when auto-detecting)\nâ€¢ Processing method used\n\nHelpful for testing which method works best for your language.\nDisable for cleaner console output in production."
                })
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("normalized_text",)
    FUNCTION = "normalize_text"
    CATEGORY = "TTS Audio Suite/Text"

    # Unicode decomposition mappings for common special characters
    DECOMPOSITION_MAP = {
        # Polish
        "Ä…": "a\u0328", "Ä™": "e\u0328", "Ä‡": "c\u0301", "Å„": "n\u0301",
        "Å›": "s\u0301", "Åº": "z\u0301", "Å¼": "z\u0307", "Ã³": "o\u0301",
        "Ä„": "A\u0328", "Ä˜": "E\u0328", "Ä†": "C\u0301", "Åƒ": "N\u0301",
        "Åš": "S\u0301", "Å¹": "Z\u0301", "Å»": "Z\u0307", "Ã“": "O\u0301",

        # German
        "Ã¤": "a\u0308", "Ã¶": "o\u0308", "Ã¼": "u\u0308", "ÃŸ": "ss",
        "Ã„": "A\u0308", "Ã–": "O\u0308", "Ãœ": "U\u0308",

        # French
        "Ã ": "a\u0300", "Ã¡": "a\u0301", "Ã¢": "a\u0302", "Ã£": "a\u0303", "Ã¤": "a\u0308",
        "Ã§": "c\u0327", "Ã¨": "e\u0300", "Ã©": "e\u0301", "Ãª": "e\u0302", "Ã«": "e\u0308",
        "Ã¬": "i\u0300", "Ã­": "i\u0301", "Ã®": "i\u0302", "Ã¯": "i\u0308",
        "Ã²": "o\u0300", "Ã³": "o\u0301", "Ã´": "o\u0302", "Ãµ": "o\u0303", "Ã¶": "o\u0308",
        "Ã¹": "u\u0300", "Ãº": "u\u0301", "Ã»": "u\u0302", "Ã¼": "u\u0308",
        "Ã€": "A\u0300", "Ã": "A\u0301", "Ã‚": "A\u0302", "Ãƒ": "A\u0303", "Ã„": "A\u0308",
        "Ã‡": "C\u0327", "Ãˆ": "E\u0300", "Ã‰": "E\u0301", "ÃŠ": "E\u0302", "Ã‹": "E\u0308",
        "ÃŒ": "I\u0300", "Ã": "I\u0301", "ÃŽ": "I\u0302", "Ã": "I\u0308",
        "Ã’": "O\u0300", "Ã“": "O\u0301", "Ã”": "O\u0302", "Ã•": "O\u0303", "Ã–": "O\u0308",
        "Ã™": "U\u0300", "Ãš": "U\u0301", "Ã›": "U\u0302", "Ãœ": "U\u0308",

        # Spanish
        "Ã±": "n\u0303", "Ã‘": "N\u0303",

        # Portuguese (additional to French)
        "Ä": "a\u0304", "Ä“": "e\u0304", "Ä«": "i\u0304", "Å": "o\u0304", "Å«": "u\u0304",

        # Czech
        "Ä": "c\u030C", "Ä": "d\u030C", "Ä›": "e\u030C", "Åˆ": "n\u030C", "Å™": "r\u030C",
        "Å¡": "s\u030C", "Å¥": "t\u030C", "Å¯": "u\u030A", "Ã½": "y\u0301", "Å¾": "z\u030C",
        "ÄŒ": "C\u030C", "ÄŽ": "D\u030C", "Äš": "E\u030C", "Å‡": "N\u030C", "Å˜": "R\u030C",
        "Å ": "S\u030C", "Å¤": "T\u030C", "Å®": "U\u030A", "Ã": "Y\u0301", "Å½": "Z\u030C",

        # Nordic languages
        "Ã¦": "ae", "Ã¸": "o\u0338", "Ã¥": "a\u030A",
        "Ã†": "AE", "Ã˜": "O\u0338", "Ã…": "A\u030A",
    }

    # Character mappings for ASCII fallbacks
    ASCII_MAP = {
        # Polish
        "Ä…": "a", "Ä™": "e", "Ä‡": "c", "Å„": "n", "Å›": "s", "Åº": "z", "Å¼": "z", "Ã³": "o", "Å‚": "l",
        "Ä„": "A", "Ä˜": "E", "Ä†": "C", "Åƒ": "N", "Åš": "S", "Å¹": "Z", "Å»": "Z", "Ã“": "O", "Å": "L",

        # German
        "Ã¤": "ae", "Ã¶": "oe", "Ã¼": "ue", "ÃŸ": "ss",
        "Ã„": "Ae", "Ã–": "Oe", "Ãœ": "Ue",

        # French, Spanish, Portuguese, Italian
        "Ã ": "a", "Ã¡": "a", "Ã¢": "a", "Ã£": "a", "Ã¤": "a", "Ã¥": "a",
        "Ã§": "c", "Ã¨": "e", "Ã©": "e", "Ãª": "e", "Ã«": "e",
        "Ã¬": "i", "Ã­": "i", "Ã®": "i", "Ã¯": "i",
        "Ã±": "n", "Ã²": "o", "Ã³": "o", "Ã´": "o", "Ãµ": "o", "Ã¶": "o", "Ã¸": "o",
        "Ã¹": "u", "Ãº": "u", "Ã»": "u", "Ã¼": "u",
        "Ã½": "y", "Ã¿": "y",

        # Uppercase versions
        "Ã€": "A", "Ã": "A", "Ã‚": "A", "Ãƒ": "A", "Ã„": "A", "Ã…": "A",
        "Ã‡": "C", "Ãˆ": "E", "Ã‰": "E", "ÃŠ": "E", "Ã‹": "E",
        "ÃŒ": "I", "Ã": "I", "ÃŽ": "I", "Ã": "I",
        "Ã‘": "N", "Ã’": "O", "Ã“": "O", "Ã”": "O", "Ã•": "O", "Ã–": "O", "Ã˜": "O",
        "Ã™": "U", "Ãš": "U", "Ã›": "U", "Ãœ": "U",
        "Ã": "Y", "Å¸": "Y",

        # Czech
        "Ä": "c", "Ä": "d", "Ä›": "e", "Åˆ": "n", "Å™": "r", "Å¡": "s", "Å¥": "t", "Å¯": "u", "Å¾": "z",
        "ÄŒ": "C", "ÄŽ": "D", "Äš": "E", "Å‡": "N", "Å˜": "R", "Å ": "S", "Å¤": "T", "Å®": "U", "Å½": "Z",

        # Nordic
        "Ã¦": "ae", "Ã†": "AE",
    }

    def detect_language(self, text: str) -> str:
        """Detect language from text based on character patterns"""
        # Character sets for different languages
        language_chars = {
            'polish': 'Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»',
            'german': 'Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ',
            'french': 'Ã Ã¢Ã¦Ã§Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã¹Ã»Ã€Ã‚Ã†Ã‡Ã‰ÃˆÃŠÃ‹ÃŽÃÃ”Ã™Ã›',
            'spanish': 'Ã¡Ã©Ã­Ã±Ã³ÃºÃ¼ÃÃ‰ÃÃ‘Ã“ÃšÃœ',
            'portuguese': 'Ã Ã¡Ã¢Ã£Ã§Ã©ÃªÃ­Ã³Ã´ÃµÃºÃ€ÃÃ‚ÃƒÃ‡Ã‰ÃŠÃÃ“Ã”Ã•Ãš',
            'italian': 'Ã Ã¨Ã©Ã¬Ã­Ã®Ã²Ã³Ã¹ÃºÃ€ÃˆÃ‰ÃŒÃÃŽÃ’Ã“Ã™Ãš',
            'czech': 'Ã¡ÄÄÃ©Ä›Ã­ÅˆÃ³Å™Å¡Å¥ÃºÅ¯Ã½Å¾ÃÄŒÄŽÃ‰ÄšÃÅ‡Ã“Å˜Å Å¤ÃšÅ®ÃÅ½',
            'slovak': 'Ã¡Ã¤ÄÄÃ©Ã­Ä¾ÄºÅˆÃ³Ã´Å•Å¡Å¥ÃºÃ½Å¾ÃÃ„ÄŒÄŽÃ‰ÃÄ½Ä¹Å‡Ã“Ã”Å”Å Å¤ÃšÃÅ½',
            'hungarian': 'Ã¡Ã©Ã­Ã³Ã¶Å‘ÃºÃ¼Å±ÃÃ‰ÃÃ“Ã–ÅÃšÃœÅ°',
            'norwegian': 'Ã¦Ã¸Ã¥Ã†Ã˜Ã…',
            'swedish': 'Ã¤Ã¶Ã¥Ã„Ã–Ã…',
            'danish': 'Ã¦Ã¸Ã¥Ã†Ã˜Ã…',
            'finnish': 'Ã¤Ã¶Ã¥Ã„Ã–Ã…',
            'dutch': ''  # Uses mostly standard Latin
        }

        # Count character matches for each language
        scores = {}
        for lang, chars in language_chars.items():
            if chars:  # Skip empty character sets
                score = sum(1 for char in text if char in chars)
                if score > 0:
                    scores[lang] = score

        # Return language with highest score, default to English
        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        return 'english'

    def apply_unicode_decomposition(self, text: str) -> str:
        """Apply Unicode decomposition to special characters"""
        result = ""
        for char in text:
            if char in self.DECOMPOSITION_MAP:
                result += self.DECOMPOSITION_MAP[char]
            else:
                result += char
        return result

    def apply_character_mapping(self, text: str) -> str:
        """Apply ASCII character mapping as fallback"""
        result = ""
        for char in text:
            if char in self.ASCII_MAP:
                result += self.ASCII_MAP[char]
            else:
                result += char
        return result

    def apply_ipa_phonemization(self, text: str, language: str) -> str:
        """Apply IPA phonemization using the same method as the reference Polish F5-TTS"""
        import re

        # Try standard phonemizer first (Linux/Mac/standard)
        try:
            from phonemizer import phonemize

            # Map language names to codes (same as reference)
            lang_codes = {
                'polish': 'pl', 'german': 'de', 'french': 'fr', 'spanish': 'es',
                'portuguese': 'pt', 'italian': 'it', 'czech': 'cs', 'slovak': 'sk',
                'hungarian': 'hu', 'norwegian': 'no', 'swedish': 'sv', 'danish': 'da',
                'finnish': 'fi', 'dutch': 'nl', 'english': 'en'
            }

            lang_code = lang_codes.get(language.lower(), 'en')

            # Use exact same parameters as reference implementation
            ipa_text = phonemize(
                text,
                language=lang_code,
                backend='espeak',
                strip=False,
                preserve_punctuation=True,
                with_stress=True
            )

            # Apply same cleanup as reference implementation
            # Remove language markings like (en), (cmn), (de), (pl), (ru)
            ipa_text = re.sub(r'\([a-z]{2,3}\)', '', ipa_text)
            ipa_text = re.sub(r'tÊƒËˆaÉªniËzlËˆeÌžtÉ™', '', ipa_text)
            ipa_text = re.sub(r'tÊƒËˆaÉªniËzÉ­ËˆetÉ™', '', ipa_text)
            ipa_text = re.sub(r'dÊ’ËˆapÉ™niËzlËˆeÌžtÉ™', '', ipa_text)
            ipa_text = re.sub(r'dÊ’ËˆapÉ™niËzÉ­ËˆetÉ™', '', ipa_text)

            return ipa_text

        except Exception as e1:
            # Try Windows-specific espeak-phonemizer-windows as fallback
            try:
                from espeak_phonemizer import Phonemizer

                # Map language names to espeak voice codes
                voice_map = {
                    'polish': 'pl', 'german': 'de', 'french': 'fr', 'spanish': 'es',
                    'portuguese': 'pt', 'italian': 'it', 'czech': 'cs', 'slovak': 'sk',
                    'hungarian': 'hu', 'norwegian': 'no', 'swedish': 'sv', 'danish': 'da',
                    'finnish': 'fi', 'dutch': 'nl', 'english': 'en'
                }

                voice = voice_map.get(language.lower(), 'en')
                phonemizer = Phonemizer()
                ipa_text = phonemizer.phonemize(text, voice=voice)

                # Apply same cleanup as reference implementation
                ipa_text = re.sub(r'\([a-z]{2,3}\)', '', ipa_text)
                ipa_text = re.sub(r'tÊƒËˆaÉªniËzlËˆeÌžtÉ™', '', ipa_text)
                ipa_text = re.sub(r'tÊƒËˆaÉªniËzÉ­ËˆetÉ™', '', ipa_text)
                ipa_text = re.sub(r'dÊ’ËˆapÉ™niËzlËˆeÌžtÉ™', '', ipa_text)
                ipa_text = re.sub(r'dÊ’ËˆapÉ™niËzÉ­ËˆetÉ™', '', ipa_text)

                return ipa_text

            except Exception as e2:
                print(f"âš ï¸ Both phonemizers failed - standard: {e1}, windows: {e2}")
                print("âš ï¸ Falling back to Unicode decomposition")
                return self.apply_unicode_decomposition(text)

    def normalize_text(self, text: str, method: str, language: str, show_debug: bool = True) -> Tuple[str]:
        """Main normalization function"""
        if not text.strip():
            return (text,)

        original_text = text

        # Auto-detect language if requested
        if language == "Auto-detect":
            detected_lang = self.detect_language(text)
            if show_debug:
                print(f"ðŸ” Auto-detected language: {detected_lang}")
        else:
            detected_lang = language.lower()

        # Apply selected method
        if method == "Pass-through":
            normalized_text = text

        elif method == "Unicode Decomposition":
            normalized_text = self.apply_unicode_decomposition(text)

        elif method == "IPA Phonemization":
            normalized_text = self.apply_ipa_phonemization(text, detected_lang)

        elif method == "Character Mapping":
            normalized_text = self.apply_character_mapping(text)

        else:
            # Fallback to pass-through
            normalized_text = text

        # Show debug information if requested
        if show_debug and normalized_text != original_text:
            print(f"ðŸ“ Phoneme Text Normalizer")
            print(f"   Method: {method}")
            print(f"   Language: {detected_lang if language == 'Auto-detect' else language}")
            print(f"   Original:   '{original_text[:100]}{'...' if len(original_text) > 100 else ''}'")
            print(f"   Normalized: '{normalized_text[:100]}{'...' if len(normalized_text) > 100 else ''}'")

            # Show character-by-character changes for short texts (only for same-length transformations)
            if len(original_text) <= 50 and len(original_text) == len(normalized_text):
                changes = []
                for i, (orig, norm) in enumerate(zip(original_text, normalized_text)):
                    if orig != norm:
                        changes.append(f"{orig}â†’{norm}")
                if changes:
                    print(f"   Changes: {', '.join(changes)}")
            elif len(original_text) != len(normalized_text) and len(original_text) <= 50:
                print(f"   Text length changed: {len(original_text)} â†’ {len(normalized_text)} characters")

        return (normalized_text,)


# Register the node
NODE_CLASS_MAPPINGS = {
    "PhonemeTextNormalizer": PhonemeTextNormalizer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PhonemeTextNormalizer": "ðŸ“ Phoneme Text Normalizer"
}